The Second-stage distillation 
두번째 단계에서는 student model이 더 좋은 성능을 갖게하는 방법입니다.  self로 학습하게 합니다
이러한 방식으로 distillation을 통해 처음부터 학습이 되었는지 여부에 관계없이 student model을 개선할 수 있습니다. 

pose estimator는 encoder, decoder 구조로 구성됩니다.
우선, train된 backbone과 그렇지 않은 head로 student를 만들고, 같은 학습된 backbone과 head로 teacher를 준비합니다.
그리고, student의 backbone은 freeze하고 head만 학습시에 사용합니다. 

teacher와 student는 같은 구조리기 때문에 backbone에서 한번만 feature를 뽑으면 되고, 그 feature를 stuedent와 teacher의 head에 입력하여 logit Si와Ti를 얻어 냅니다. 

식 3에 따라 2단계 distillation을 위해 logit L_{logit}으로 student를 학습합니다. == logit-distillation에서 사용했던 같은 방식의 loss를 적용하는 것
이렇게 되면 label로 계산된 값을 L_ori를 떨어뜨릴 수 있습니다.

loss scale에 대한 하이퍼파라미터를 나타내기 위해 γ 를 사용하면 second-stage distillation의 최종 손실은 다음과 같이 공식화 할 수 있습니다. 

그림
이는 기존의 self-KD 방법과 달리 head-aware distillation 방법은 단 20%의 학습 시간으로 head에서 지식을 효율적으로 추출하고 localization capability를 더욱 향상시킬 수 있습니다.
