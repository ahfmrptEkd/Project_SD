먼저, 부착된 네트워크 모듈의 가중치는 모두 0이므로, 새 모델은 학습되어 잠긴 모델을 활용할 수 있습니다.

학습중에는 학습용 이미지와 함께 두개의 조건부여(conditioning)가 공급됩니다. 첫번째는 텍스트 프롬프트, 두번째는 Openpose 키포인트나 Canny외곽선과 같은 전처리 자료입니다. 이런 방식으로 이들 두 가지 입력에 기초한 이미지를 생성하도록 학습할 수 있습니다. 이때, 각각의 전처리별 방법은 독립적으로 학습받게 됩니다.
==
잠긴 블록🔒과 학습가능한 블록🔓을 만든다.
1.학습 가능한 블록으로만 학습하고, 잠긴 블록을 통해 기존 모델을 보존한다.
2.이를 통해 원래 모델에서 파괴나 왜곡이 일어나지 않아 안전하다.
3.👉 때문에 더 좋은 결과를 낼 수 있다.


ControlNet을 간단하게 묘사하면 다음과 같다.

diffusion model의 parameter를 복사하여 새로운 학습 프레임워크를 원래 parameter와 병렬로 구성한다. 이를 각각 “trainable(학습 가능한) copy”와 “locked(학습 불가능한) copy”라고 부른다.
Locked copy는 기존 network의 성능인 이미지 생성에 필요한 representation을 유지하고 있다고 생각할 수 있다.
Trainable copy는 conditional control을 위해 여러 task-specific dataset에 대해 학습되는 프레임워크다.
Locked copy와 Trainable copy는 zero convolution을 통해 서로 연결된다. Zero convolution 또한 학습 가능한 레이어에 속한다.
대충만 쭉 묘사했는데 사실 이 부분은 그림을 보면 이해가 쉽다.

x가 들어가서 y가 나오는 구조는 diffusion process에 접목시키게 되면 특정 시점의 noised latent vector 
z_t가 input으로 들어가서 다음 시점의 noised latent vector 
z_t−1를 예측하는 것과 같다. 회색으로 된 neural network는 원래의 diffusion model로 파라미터가 고정된 채 변하지 않게끔 하면 사전 학습된 디퓨전 모델의 이미지를 만드는 성능을 해치지 않고 가만히 놔둘 수 있다.

좌측의 얼어있는 친구는 가만 놔두고 우측의 불타는 친구만 condition에 대해 학습한다고 생각하면 된다. Trainable copy이므로 fine-tuning 과정인데 원래의 parameter를 최대한 손상시키기 않겠다는 의도가 보이는 학습 구조가 된다.