CLIP 모델은 레이어로 구성된 구조를 가지고 있습니다. 각 레이어는 이전 레이어보다 더 구체적입니다. 
예를 들어 레이어 1이 "사람"이라면 레이어 2는 "남성", "여성"이 될 수 있습니다: "남성" 및 "여성"이 될 수 있고, "남성"의 경로를 따라가면 레이어 3이 될 수 있습니다: 남자, 소년, 청년, 아버지, 할아버지... 등이 될 수 있습니다. 
이 방식이 클립 모델의 정확한 구조는 아니지만, 예를 들어 설명하기 위한 것입니다.

예를 들어 1.5 모델의 깊이는 12랭크입니다. 여기서 12번째 레이어는 텍스트 임베딩의 마지막 레이어입니다. 각 레이어에는 일정한 크기의 매트릭스가 있고, 각 레이어에는 추가 매트릭스가 있습니다. 따라서 4x4 첫 번째 레이어에는 그 아래에 4x4가 4개 있고... 등등. 따라서 텍스트 공간은 차원적으로 엄청나게 큽니다.

이제 클립 레이어에서 왜 더 일찍 멈추고 싶을까요? "소"의 그림을 원한다면 텍스트 모델에 포함될 수 있는 "소"의 하위 카테고리는 신경 쓰지 않을 수 있습니다. 특히 품질이 다양할 수 있기 때문입니다. 따라서 '소'를 원한다면 '아베더딘 앵거스 황소'는 원하지 않을 수 있습니다.

클립 건너뛰기는 기본적으로 "텍스트 모델을 얼마나 정확하게 만들 것인지"에 대한 설정이라고 생각하면 됩니다. 예를 들어 XY 스크립트로 테스트해 볼 수 있습니다. 각 클립 단계마다 설명에 더 많은 정의가 있다는 것을 알 수 있습니다. 예를 들어 들판에 서 있는 젊은 남성에 대한 자세한 설명이 있는 경우 클립 단계가 낮을수록 "서 있는 남자", 더 깊게는 "서 있는 젊은 남자", "숲에 서 있는 젊은 남자" 등의 이미지가 표시될 수 있습니다.

클립 건너뛰기는 특별한 방식으로 구조화된 모델을 사용할 때 정말 유용합니다. 부루 모델처럼요. '1girl' 태그가 하나의 주요 태그에 연결되는 여러 개의 하위 태그로 세분화될 수 있습니다. 클립 건너뛰기의 활용 여부는 시행착오에 따라 달라질 수 있습니다.

클립 건너뛰기는 클립을 사용하거나 클립을 사용하는 모델을 기반으로 하는 모델에서만 작동한다는 점을 기억하세요. 1.x 모델과 그 파생 모델에서와 같이. 2.0 모델 및 파생 모델은 OpenCLIP을 사용하기 때문에 CLIP과 상호 작용하지 않습니다.

--------------------------------------------
먼저 CLIP 모델에서 그 프롬프트를 77개의 토큰 list로 변환하고

12개의 transformer encoding layer를 거쳐서 그 토큰들은 77*768 임베딩으로 변환한다.

그리고 u-net이 그 77*768 임베딩을 입력값으로 받아 그림을 그린다. 이 임베딩이 그림을 특정 방향으로 이끄는 conditioning이다.
근데 프롬프트에 1girl만 입력하면 그 프롬프트는 [272, 1611]이라는 2개밖에 안되는 토큰으로 변환된다.

이게 어떻게 77 크기로 확장시키는 것일까?
CLIP은 최대 75토큰의 입력값을 받아서 가장 앞에 BOS(49406) 토큰을 넣고, 뒤에는 EOS(49407) 토큰으로 채운다.

즉 '1girl'을 프롬프트로 넣으면 이것을 [49406, 272, 1611, 49407, 0, 0, ..., 0] 토큰으로 변환하게 되는 것이다.
일반적인 텍스트 기반 transformer 모델들은 attention mask라는 것을 사용하여, 토큰 간의 attention을 계산할 때 padding token이 고려되는 것을 막는다.

이 그림만 보면 T1, T2 이것들이 각 토큰에 해당하는 임베딩이라고 생각하게 되는데 아니다.

실제로 CLIP은 한 텍스트 구절에 대해 가장 왼쪽에 있는 EOS 토큰 위치의 임베딩만 뽑고 거기에 Linear layer 만 하나 더 추가시켜서 1*768 크기의 feature 임베딩을 뽑는다.

이 feature 임베딩이 그 텍스트 전체의 함축적인 의미를 담고 있는, CLIP이 구하고 싶어하는 정보이다.
그 외의 임베딩들은 feature 임베딩을 뽑기 위한 중간생성물일 뿐이다.
12개의 CLIP 레이어 중 마지막 레이어에서 각 토큰 텍스트들의 의미가 하나의 feature 임베딩에 맞춰지도록 압축되어, 토큰의 본래 의미가 사라지기 때문이라고 한다.

