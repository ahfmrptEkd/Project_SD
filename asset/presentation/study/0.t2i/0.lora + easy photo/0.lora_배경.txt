최근 chatGPT 열풍에 이어 LLM에 대한 인기가 매우 뜨겁다.
여기서 문제점으로 언급된 것이 fine-tuning할 때 모델의 파라미터가 너무 많다보니 리소스 제약이 크다는 점이였다.

 
LoRA 논문은 기존의 파라미터보다 훨씬 적은 파라미터를 가지고 튜닝하는 방법으로 통해서 리소스 제약에서 벗어나고 성능도 비슷하거나 더 높은 수준으로 훈련시킬 수 있도록 하는 수학적인 메커니즘을 소개했다.

t2i에 대해서 나온 방법론이였다.